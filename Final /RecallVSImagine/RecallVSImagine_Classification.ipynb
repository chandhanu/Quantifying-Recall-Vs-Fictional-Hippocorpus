{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving more Linguitic features and adding to the pre-processed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/hcV3-10.csv')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    #print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "# Process each story and calculate Sequentiality scores\n",
    "for index, row in df.iterrows():\n",
    "    story = row['story']\n",
    "    probabilities = calculate_probabilities(story, [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add probabilities to DataFrame\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "\n",
    "    # Calculate Sequentiality scores\n",
    "    for h in range(1, 6):\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df.at[index, seq_key] = df.at[index, f'probability_history_size{h}'] - df.at[index, 'probability_history_size0']\n",
    "\n",
    "# Extract existing linguistic features\n",
    "df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "df['avg_word_length'] = df['story'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)\n",
    "df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "import string\n",
    "df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "\n",
    "# Add new linguistic features\n",
    "df['avg_sentence_length'] = df['story'].apply(lambda x: np.mean([len(sentence.split()) for sentence in nltk.sent_tokenize(x)]) if nltk.sent_tokenize(x) else 0)\n",
    "df['sensory_word_count'] = df['story'].apply(lambda x: sum(word in {'see', 'hear', 'touch', 'taste', 'smell', 'sight', 'sound', 'texture', 'aroma', 'flavor'} for word in x.split()))\n",
    "df['first_person_pronoun_count'] = df['story'].apply(lambda x: sum(word.lower() in {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'} for word in x.split()))\n",
    "df['past_tense_verb_count'] = df['story'].apply(lambda x: sum(tag.startswith('VBD') for word, tag in nltk.pos_tag(nltk.word_tokenize(x))))\n",
    "df['emotion_word_count'] = df['story'].apply(lambda x: sum(word.lower() in {'happy', 'sad', 'angry', 'joyful', 'depressed', 'excited', 'fearful', 'anxious', 'content', 'disappointed'} for word in x.split()))\n",
    "df['dialogue_tag_count'] = df['story'].apply(lambda x: sum(word.lower() in {'said', 'asked', 'replied', 'shouted', 'whispered', 'murmured', 'screamed', 'yelled', 'muttered', 'uttered', 'exclaimed'} for word in x.split()))\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_csv('processed_values.csv', index=False)\n",
    "\n",
    "# Prepare the validation set\n",
    "X = df[['word_count', 'sentence_count', 'avg_word_length', 'lexical_diversity', 'punctuation_count', 'avg_sentence_length', 'sensory_word_count', 'first_person_pronoun_count', 'past_tense_verb_count', 'emotion_word_count', 'dialogue_tag_count'] + [f'probability_history_size{i}' for i in range(6)] + [f'Sequentiality_{i}' for i in range(1, 6)]]\n",
    "y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "# Splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train and validate the classification model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = clf.predict(X_val.fillna(0))\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Data processing and model training complete. File saved as 'processed_values.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Pre-Determined Validation set \n",
    "\n",
    "The validationg is pre-determined and it is not randomly sampled, but the usage of stratified split ensures the dataset's overal comprehensiveness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'test_processed_stories.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Training a RandomForest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluating the model on the validation set\n",
    "y_val_pred = rf_model.predict(X_val_poly)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, save the model\n",
    "# joblib.dump(rf_model, 'random_forest_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the best classifier model and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'test_processed_stories.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Stratified splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# RandomForest Classifier with Hyperparameter Tuning using Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluating the best model on the validation set\n",
    "y_val_pred = best_rf_model.predict(X_val_poly)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, save the best model\n",
    "# joblib.dump(best_rf_model, 'random_forest_classifier_optimized.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing all classification models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'fake.csv'\n",
    "file_path = 'test_processed_stories.csv'\n",
    "#file_path = 'processed_values.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Stratified splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    #\"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    #\"Random_GridSearchCV\": GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "    #\"Random_GridSearchCV\": GridSearchCV(RandomForestClassifier(random_state=42), param_grid, scoring='accuracy')\n",
    "}\n",
    "\n",
    "# Train, evaluate, and save each model\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier\n",
    "    clf.fit(X_train_poly, y_train)  # Polynomial features used here\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_val_pred = clf.predict(X_val_poly)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"{name} Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(clf, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "\n",
    "# Feature selection based on Random Forest importance\n",
    "rf = RandomForestClassifier(random_state=42).fit(X_train_poly, y_train)\n",
    "selector = SelectFromModel(rf, prefit=True)\n",
    "X_train_selected = selector.transform(X_train_poly)\n",
    "X_val_selected = selector.transform(X_val_poly)\n",
    "\n",
    "# Retrain and evaluate with selected features\n",
    "rf_selected = RandomForestClassifier(random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "y_val_pred_selected = rf_selected.predict(X_val_selected)\n",
    "val_accuracy_selected = accuracy_score(y_val, y_val_pred_selected)\n",
    "print(f\"Validation Accuracy with Feature Selection: {val_accuracy_selected:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing using ensemble and voting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'test_processed_stories.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Stratified splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize classifiers including Random Forest with GridSearchCV\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Random_GridSearchCV\": GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "}\n",
    "\n",
    "# Train, evaluate, and save each model\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier\n",
    "    clf.fit(X_train_poly, y_train)  # Polynomial features used here\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_val_pred = clf.predict(X_val_poly)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"{name} Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "    # Detailed classification report for error analysis\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(clf, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "\n",
    "# Feature selection based on Random Forest importance\n",
    "rf = RandomForestClassifier(random_state=42).fit(X_train_poly, y_train)\n",
    "selector = SelectFromModel(rf, prefit=True)\n",
    "X_train_selected = selector.transform(X_train_poly)\n",
    "X_val_selected = selector.transform(X_val_poly)\n",
    "\n",
    "# Retrain and evaluate with selected features\n",
    "rf_selected = RandomForestClassifier(random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "y_val_pred_selected = rf_selected.predict(X_val_selected)\n",
    "val_accuracy_selected = accuracy_score(y_val, y_val_pred_selected)\n",
    "print(f\"Validation Accuracy with Feature Selection: {val_accuracy_selected:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection increases the base acuracy by 25%\n",
    "\n",
    "Random Forest Classifier provided the base accuracy of 70% and used as base to increase the accuracy of the classifications.\n",
    "By tweaking the parameters of Random Forest classifier using GridSearchCV and StratifiedFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'test_processed_stories.csv'\n",
    "#file_path = 'processed_values_hcV3-10.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store accuracy of each model\n",
    "model_accuracies = {}\n",
    "\n",
    "# Train, evaluate, and save each model\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_poly, y_train)\n",
    "    y_val_pred = clf.predict(X_val_poly)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"{name} Validation Accuracy: {val_accuracy:.2f}\")\n",
    "    model_accuracies[name] = val_accuracy\n",
    "    joblib.dump(clf, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "\n",
    "# Creating lists of classifiers and their accuracies\n",
    "classifiers = list(model_accuracies.keys())\n",
    "accuracy_values = list(model_accuracies.values())\n",
    "\n",
    "# Generating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(classifiers, accuracy_values, marker='o', color='b')\n",
    "plt.title('Model Accuracies')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimised Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'test_processed_stories.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Stratified splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    #\"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    #\"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    #\"Random Forest-191\": RandomForestClassifier(random_state=191),\n",
    "    \"Random_GridSearchCV\": GridSearchCV(RandomForestClassifier(random_state=154), param_grid, cv=StratifiedKFold(5), scoring='accuracy'),\n",
    "    \"Random_GridSearchCV\": RandomForestClassifier(random_state=154),\n",
    "}\n",
    "\n",
    "# Dictionary to store accuracy of each classifier\n",
    "accuracy_dict = {}\n",
    "\n",
    "# Train, evaluate, and save each model\n",
    "for name, clf in classifiers.items():\n",
    "    # Fit the classifier\n",
    "    clf.fit(X_train_poly, y_train)  # Polynomial features used here\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_val_pred = clf.predict(X_val_poly)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracy_dict[name] = val_accuracy  # Store the accuracy in the dictionary\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(clf, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "\n",
    "\n",
    "# Generating the accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(accuracy_dict.keys()), list(accuracy_dict.values()), marker='o', color='b')\n",
    "plt.title('Model Accuracies')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "classifications = ['(1)Basic Classification', '(2)With Sequentiality (1-5)', '(3)RF with Linguistic Features on (2)', '(4) Optimized Random Forest']\n",
    "accuracies = [0.3, 0.42, 0.75, 0.87] # Please check the individual components from other blocks \n",
    "colors = ['blue', 'blue', 'blue', 'blue']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(classifications) - 1):\n",
    "    plt.plot(classifications[i:i+2], accuracies[i:i+2], marker='o', color=colors[i], linestyle='-', label=f'{classifications[i]} to {classifications[i+1]}')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Classification Accuracies on the Hippocorpus Dataset')\n",
    "plt.xlabel('Classification Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(classifications, rotation=45)\n",
    "plt.ylim(0, 1)  # Setting the limit for y-axis\n",
    "\n",
    "# Adding grid\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5aba8cb37ae6b3ad21916108e1664d164ee89787323036f60cdb7554a2d2744a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
