{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: tqdm in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# Install required libraries\n",
    "!pip install transformers tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. File saved as 'processed_stories.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('hcV3-10.csv')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "# Process each story\n",
    "for index, row in df.iterrows():\n",
    "    story = row['story']\n",
    "    probabilities = calculate_probabilities(story, [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add probabilities to DataFrame\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "\n",
    "    # Calculate Sequentiality scores (example: difference between history sizes 0 and 1)\n",
    "    df.at[index, 'Sequentiality'] = df.at[index, 'probability_history_size1'] - df.at[index, 'probability_history_size0']\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_csv('processed_stories.csv', index=False)\n",
    "\n",
    "print(\"Data processing complete. File saved as 'processed_stories.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base classification model with just basic linguistic features and Sequentiality results from Phase 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3\n",
      "Data processing and model training complete. File saved as 'processed_stories.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/test.csv')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    #print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "# Process each story and calculate Sequentiality scores\n",
    "for index, row in df.iterrows():\n",
    "    story = row['story']\n",
    "    probabilities = calculate_probabilities(story, [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add probabilities to DataFrame\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "\n",
    "    # Calculate Sequentiality scores\n",
    "    for h in range(1, 6):\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df.at[index, seq_key] = df.at[index, f'probability_history_size{h}'] - df.at[index, 'probability_history_size0']\n",
    "\n",
    "# Extract linguistic features (example: word count, sentence count)\n",
    "df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_csv('processed_stories.csv', index=False)\n",
    "\n",
    "# Prepare the validation set\n",
    "X = df[['word_count', 'sentence_count'] + [f'probability_history_size{i}' for i in range(6)] + [f'Sequentiality_{i}' for i in range(1, 6)]]\n",
    "y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = clf.predict(X_val.fillna(0))\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Data processing and model training complete. File saved as 'processed_stories.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont modify above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4\n",
      "Data processing and model training complete. File saved as 'processed_stories.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/test.csv')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    #print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "# Process each story and calculate Sequentiality scores\n",
    "for index, row in df.iterrows():\n",
    "    story = row['story']\n",
    "    probabilities = calculate_probabilities(story, [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add probabilities to DataFrame\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "\n",
    "    # Calculate Sequentiality scores\n",
    "    for h in range(1, 6):\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df.at[index, seq_key] = df.at[index, f'probability_history_size{h}'] - df.at[index, 'probability_history_size0']\n",
    "\n",
    "# Extract linguistic features (example: word count, sentence count)\n",
    "# You can add more features as needed\n",
    "df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "df['avg_word_length'] = df['story'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)\n",
    "df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "import string\n",
    "df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_csv('processed_stories.csv', index=False)\n",
    "\n",
    "# Prepare the validation set\n",
    "X = df[['word_count', 'sentence_count','avg_word_length', 'lexical_diversity','punctuation_count'] + [f'probability_history_size{i}' for i in range(6)] + [f'Sequentiality_{i}' for i in range(1, 6)]]\n",
    "y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = clf.predict(X_val.fillna(0))\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Data processing and model training complete. File saved as 'processed_stories.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Linguistic Features to the Data set \n",
    "1. word_count\n",
    "2. sentence_count\n",
    "3. avg_word_length\n",
    "4. lexical_diversity\n",
    "5. avg_sentence_length\n",
    "6. sensory_word_count\n",
    "7. first_person_pronoun_count\n",
    "8. past_tense_verb_count\n",
    "9. emotion_word_count\n",
    "10. dialogue_tag_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chandhanu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/chandhanu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/hcV3-10.csv')\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate probabilities\n",
    "def calculate_probabilities(text, history_sizes):\n",
    "    sentences = sent_tokenize(text)\n",
    "    probabilities = {f'probability_history_size{h}': [] for h in history_sizes}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        if tokens.size(1) > 1024:  # GPT-2's maximum context size\n",
    "            continue  # Skip this sentence or truncate it\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens, labels=tokens)\n",
    "            loss = outputs.loss\n",
    "            sentence_probability = torch.exp(-loss).item()\n",
    "\n",
    "        for h in history_sizes:\n",
    "            # Adjust the context window for each history size\n",
    "            context_size = min(h, tokens.size(1))\n",
    "            context = tokens[:, :context_size] if context_size > 0 else tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = model(context, labels=tokens[:, :context.size(1)])\n",
    "                loss = outputs.loss\n",
    "                context_probability = torch.exp(-loss).item()\n",
    "\n",
    "            probabilities[f'probability_history_size{h}'].append(context_probability)\n",
    "    #print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "# Process each story and calculate Sequentiality scores\n",
    "for index, row in df.iterrows():\n",
    "    story = row['story']\n",
    "    probabilities = calculate_probabilities(story, [0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add probabilities to DataFrame\n",
    "    for key, value in probabilities.items():\n",
    "        df.at[index, key] = sum(value) / len(value) if value else None\n",
    "\n",
    "    # Calculate Sequentiality scores\n",
    "    for h in range(1, 6):\n",
    "        seq_key = f'Sequentiality_{h}'\n",
    "        df.at[index, seq_key] = df.at[index, f'probability_history_size{h}'] - df.at[index, 'probability_history_size0']\n",
    "\n",
    "# Extract existing linguistic features\n",
    "df['word_count'] = df['story'].apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df['story'].apply(lambda x: len(sent_tokenize(x)))\n",
    "df['avg_word_length'] = df['story'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)\n",
    "df['lexical_diversity'] = df['story'].apply(lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0)\n",
    "import string\n",
    "df['punctuation_count'] = df['story'].apply(lambda x: sum(1 for char in x if char in string.punctuation))\n",
    "\n",
    "# Add new linguistic features\n",
    "df['avg_sentence_length'] = df['story'].apply(lambda x: np.mean([len(sentence.split()) for sentence in nltk.sent_tokenize(x)]) if nltk.sent_tokenize(x) else 0)\n",
    "df['sensory_word_count'] = df['story'].apply(lambda x: sum(word in {'see', 'hear', 'touch', 'taste', 'smell', 'sight', 'sound', 'texture', 'aroma', 'flavor'} for word in x.split()))\n",
    "df['first_person_pronoun_count'] = df['story'].apply(lambda x: sum(word.lower() in {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'} for word in x.split()))\n",
    "df['past_tense_verb_count'] = df['story'].apply(lambda x: sum(tag.startswith('VBD') for word, tag in nltk.pos_tag(nltk.word_tokenize(x))))\n",
    "df['emotion_word_count'] = df['story'].apply(lambda x: sum(word.lower() in {'happy', 'sad', 'angry', 'joyful', 'depressed', 'excited', 'fearful', 'anxious', 'content', 'disappointed'} for word in x.split()))\n",
    "df['dialogue_tag_count'] = df['story'].apply(lambda x: sum(word.lower() in {'said', 'asked', 'replied', 'shouted', 'whispered', 'murmured', 'screamed', 'yelled', 'muttered', 'uttered', 'exclaimed'} for word in x.split()))\n",
    "\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_csv('/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/processed_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Classification Model - with basic data set sample (100 entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.42\n",
      "Data processing and model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Correcting Feature Set Preparation\n",
    "feature_columns = [\n",
    "    'word_count', 'sentence_count', 'avg_word_length', 'lexical_diversity', \n",
    "    'punctuation_count', 'avg_sentence_length', 'sensory_word_count', \n",
    "    'first_person_pronoun_count', 'past_tense_verb_count', 'emotion_word_count', \n",
    "    'dialogue_tag_count'\n",
    "] + [f'probability_history_size{i}' for i in range(6)] + [f'Sequentiality_{i}' for i in range(1, 6)]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['memType']  # Assuming 'memType' is the column indicating recalled or imagined\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Train the RandomForest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = clf.predict(X_val.fillna(0))\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Data processing and model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Classification model - 1000 entries with tweaked classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/w4jkwn2n5399nzpj8v7tvn3c0000gn/T/ipykernel_43309/947184936.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
      "/var/folders/gf/w4jkwn2n5399nzpj8v7tvn3c0000gn/T/ipykernel_43309/947184936.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Feature Selection: 0.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/chandhanu/Documents/GitHub/Topics-in-AI-Project-598/processed_values_hcV3-10.csv'\n",
    "#file_path = 'test_processed_stories.csv'\n",
    "#file_path = 'test_processed_stories.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "selected_features = [\n",
    "    \"stressful\", \"probability_history_size0\", \"probability_history_size2\",\n",
    "    \"probability_history_size3\", \"probability_history_size4\", \"probability_history_size5\",\n",
    "    \"Sequentiality_2\", \"Sequentiality_3\", \"Sequentiality_4\", \"Sequentiality_5\",\n",
    "    \"word_count\", \"sentence_count\", \"avg_word_length\", \"lexical_diversity\",\n",
    "    \"punctuation_count\", \"avg_sentence_length\", \"sensory_word_count\",\n",
    "    \"first_person_pronoun_count\", \"past_tense_verb_count\", \"emotion_word_count\",\n",
    "    \"dialogue_tag_count\"\n",
    "]\n",
    "\n",
    "target_column = 'memType'\n",
    "class_mapping = {'imagined': 0, 'recalled': 1, 'retold': 1}\n",
    "\n",
    "# Preprocessing data\n",
    "relevant_data = data[selected_features + [target_column]]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "relevant_data[selected_features] = imputer.fit_transform(relevant_data[selected_features])\n",
    "\n",
    "# Map 'memType' to binary classes\n",
    "relevant_data[target_column] = relevant_data[target_column].map(class_mapping)\n",
    "\n",
    "# Splitting the dataset into features (X) and target (y)\n",
    "X = relevant_data[selected_features]\n",
    "y = relevant_data[target_column]\n",
    "\n",
    "# Stratified splitting data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None, 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize classifiers including Random Forest with GridSearchCV\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Random_GridSearchCV\": GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "}\n",
    "# Feature selection based on Random Forest importance\n",
    "rf = RandomForestClassifier(random_state=42).fit(X_train_poly, y_train)\n",
    "selector = SelectFromModel(rf, prefit=True)\n",
    "X_train_selected = selector.transform(X_train_poly)\n",
    "X_val_selected = selector.transform(X_val_poly)\n",
    "\n",
    "# Retrain and evaluate with selected features\n",
    "rf_selected = RandomForestClassifier(random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "y_val_pred_selected = rf_selected.predict(X_val_selected)\n",
    "val_accuracy_selected = accuracy_score(y_val, y_val_pred_selected)\n",
    "print(f\"Validation Accuracy with Feature Selection: {val_accuracy_selected:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Classification.ipynb for more classification modifications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5aba8cb37ae6b3ad21916108e1664d164ee89787323036f60cdb7554a2d2744a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
